\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[hyphens]{url}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Ensure letter paper
\pdfpagewidth=8.5in
\pdfpageheight=11in

%%%%%%%%%%%---SETME-----%%%%%%%%%%%%%
\newcommand{\iscasubmissionnumber}{NaN}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagenumbering{arabic}

%%%%%%%%%%%---SETME-----%%%%%%%%%%%%%
\title{Literature Survey: Vector Instruction Sets
}
\author{\normalsize{ISCA 2023 Submission
    \textbf{\#\iscasubmissionnumber} -- Confidential Draft -- Do NOT Distribute!!}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\author{\IEEEauthorblockN{1\textsuperscript{st} Shao-Chien Chiang}
\IEEEauthorblockA{\textit{Dept. of Electrical and Computer Engr.} \\
\textit{Texas A\&M University}\\
College Station, TX \\
}
\\

\IEEEauthorblockN{3\textsuperscript{rd} Emily Chang}
\IEEEauthorblockA{\textit{Dept. of Electrical and Computer Engr.} \\
\textit{Texas A\&M University}\\
College Station, TX \\
}
\and

\IEEEauthorblockN{2\textsuperscript{nd} Hsin-An Ko}
\IEEEauthorblockA{\textit{Dept. of Electrical and Computer Engr.} \\
\textit{Texas A\&M University}\\
College Station, TX \\
}
\\
\IEEEauthorblockN{4\textsuperscript{th} Pin-Tzu Huang}
\IEEEauthorblockA{\textit{Dept. of Electrical and Computer Engr.} \\
\textit{Texas A\&M University}\\
College Station, TX \\
}
}


\maketitle
\thispagestyle{plain}
\pagestyle{plain}



%%%%%% -- PAPER CONTENT STARTS-- %%%%%%%%

\begin{abstract}

% This document is intended to serve as a sample for submissions to the
% 50th IEEE/ACM International Symposium on Computer Architecture (ISCA),
% June 17--23, 2023 in Orlando, FL, USA. This document provides
% guidelines that authors should follow when submitting papers to the
% conference. This format is derived from the IEEE conference template IEEEtran.cls
% file with the objective of keeping the submission similar to the final
% version, i.e., the IEEEtran.cls template will also be used for
% the camera-ready version.

\end{abstract}
%%% ====================== Shao-Chien Chiang =====================
\section{Introduction}
After the introduction of the iconic CRAY-1\cite{b8} machine in 1976, vector processors dominated supercomputers, but declined in the 1990s. However, their design principles continued to influence future ISA extensions. 
As the demands for multimedia and graphics grew, many commodity microprocessors introduced SIMD extensions to support these computations\cite{b9}. These extensions consist of short vector instructions that operate on packed data in registers. Early implementations used existing microarchitecture with insignificant changes\cite{b12}. They partitioned the ALU to support subword operations, or remapped floating-point registers to new SIMD registers. Those extensions also included MMX\cite{b10} for Intel x86, VIS\cite{b11} for UltraSPARC, and MDMX for MIPS. 
While overloading registers maintained compatibility with existing OS context switch routines, it also hindered simultaneous use of floating-point and SIMD operations. It increased the complexity of both hardware and software to accommodate shared registers for future extensions. SSE\cite{b13} for x86 and AltiVec\cite{b14} for PowerPC introduced independent 128-bit registers to address this issue. 
SIMD extensions came with several issues\cite{b15,b16}. Some of these issues still exist these days. The first challenge is data alignment. Hardware needed mechanisms to handle SIMD instructions that accessed data across page boundaries. For software, we had to handle situations where the data length was not a multiple of the vector length, or where unaligned memory accesses could cause significant performance impact. Another issue was that it was difficult to find a general way to optimize codes because optimizations were dependent on the features of each extension. Compilers could not generate efficient vectorization instructions. To fully utilize the power of SIMD, many programmers wrote their vector codes in handwritten assembly. Research\cite{b17} showed that only a small portion of programs executed SIMD calculations, while 75–80% consisted of overhead or supporting instructions. 
SVE\cite{b7} for ARM introduced several innovations to address these problems. By leaving the vector length as an implementation choice, its program model allowed software to run without needing to recompile to support the available vector length. SVE also introduced the ability to perform predicted operations. This prediction was useful in reducing the overhead and supporting instructions mentioned earlier. Similarly, AVX-512 from x86 also supports predicated instructions. The introduction of first-fault mechanisms enabled fault-tolerant speculative vectorization, suppressing memory faults and indicating which elements were unsuccessfully loaded. The new horizontal operations solved the problems of dependencies across vectors. These designs also improved compiler auto-vectorization. Meanwhile, the RISC-V vector extension introduced a distinct design. Its vector length could dynamically vary at runtime. 
In addition to their initial use in multimedia and graphics, vector extensions can enhance applications across various fields, with designs adopting concepts from traditional vector processors, such as variable vector lengths and predicated instructions.


%%% ===============================================================
%%% ===================== Hsin-An Ko ==============================
\section{Architecture Design and Implementation}
Vector instruction sets play a crucial role in modern computing architectures by 
enabling efficient parallel processing. In today's mainstream computer architectures, 
vector instruction sets can be broadly categorized into three major types: 
the AVX instruction set, which was introduced early and continues to be updated 
for the x86 architecture; the more recently introduced SVE instruction set for the ARM architecture; 
and the vector instruction set provided by RISC-V, the most popular open architecture, 
enabling broader application scenarios.

The following discussion will be divided into three sections:

I. Compare the design of vector instruction sets across different architectures.
II. Examine architectural design features.
III. Analyze the trade-offs in instruction set design.


\subsection{Compare the design of vector instruction sets across different architectures.}
(a) x86 AVX vs. ARM SVE

One of the fundamental differences between AVX and SVE lies in their approach to vector width. 
AVX employs fixed vector widths of 128-bit (AVX), 256-bit (AVX2), and 512-bit (AVX-512)\cite{b2}. 
In contrast, SVE supports scalable vector lengths ranging from 128-bit to 2048-bit, 
allowing runtime adaptation and greater flexibility\cite{b4}. 
This scalable approach in SVE enables more efficient use of hardware resources, as the same binary 
can run on different hardware implementations without modification.

Masking mechanisms further differentiate AVX and SVE. AVX-512 incorporates explicit mask registers (K0-K7) to enable selective operations on vector elements, which enhances performance in specific computational workloads\cite{b2}. However, SVE takes a different approach by utilizing predicate registers for lane masking, which offers more dynamic and flexible control over which elements participate in computation\cite{b5}. This design simplifies compiler optimizations and increases execution efficiency.

Memory access is another crucial aspect where the two architectures diverge. AVX supports explicit gather and scatter operations, but it requires multiple instructions for optimal utilization, making it somewhat complex in certain workloads\cite{b1}. In contrast, SVE integrates gather-load and scatter-store operations with predicate-based control, significantly improving memory efficiency\cite{b5}. This built-in flexibility enables better handling of non-contiguous memory accesses, which is particularly beneficial for high-performance computing applications.

(b) RISC-V Vector Extensions (RVV)

RISC-V Vector Extensions (RVV) introduce a vector-length-agnostic (VLA) approach similar to SVE, where vector width is hardware-dependent and can scale dynamically\cite{b6}. This feature allows different implementations to optimize vector execution based on available resources, enhancing performance across a variety of hardware configurations.

Unlike AVX and SVE, which define specific vector widths, RVV provides a high level of customizability. Implementations can define specific vector register widths and instruction subsets, making it highly adaptable for domain-specific applications such as embedded systems and AI accelerators\cite{b6}. This flexibility ensures that RISC-V processors can cater to diverse application needs while maintaining efficiency.

Memory access and masking in RVV share similarities with SVE. It supports various access patterns, including strided, indexed, and unit-stride memory operations, with built-in masking mechanisms to control element participation\cite{b6}. This capability enhances the efficiency of vectorized operations and simplifies programming, particularly for workloads that require irregular memory access patterns.
\subsection{Examine architectural design features.}
(a) Impact of Vector Width on Performance

Increasing vector width significantly boosts computational throughput but also introduces challenges in power consumption and architectural complexity. Studies show that expanding vector width in SVE from 128-bit to 512-bit improves performance by up to 38\% while reducing energy consumption by 23\%\cite{b4}. However, beyond 512-bit, the benefits diminish due to memory bandwidth limitations, which hinder further performance gains\cite{b5}.

(b) Hardware Implementation of Variable-Length Vectors

The implementation of vector execution also differs across these architectures. AVX relies on fixed-length execution units, requiring separate ALUs for 128-bit, 256-bit, and 512-bit operations. This design increases chip area and power consumption, particularly when handling mixed-width operations\cite{b3}. In contrast, SVE and RVV utilize a single vector unit that dynamically adapts to available vector width, reducing hardware redundancy and improving efficiency\cite{b5}

\subsection{ Analyze the trade-offs in instruction set design.}
(a) Power Consumption

Power efficiency is a critical consideration in vector instruction design. AVX consumes substantial power due to its wide registers and fixed-length execution units. AVX-512, in particular, is known to reduce CPU clock speeds under heavy load to mitigate power draw, which can impact performance in sustained workloads\cite{b3}. On the other hand, SVE and RVV offer dynamic vector length adaptation, enabling better power efficiency by optimizing resource usage based on workload demands\cite{b4}.

(b) Latency and Throughput Considerations

From a latency and throughput perspective, AVX provides high throughput but incurs overhead in mixed-width operations, requiring additional processing steps\cite{b1}. SVE, with its flexible masking and integrated gather/scatter support, reduces latency and improves computational efficiency\cite{b5}. Meanwhile, RVV balances flexibility with instruction complexity, making its performance heavily dependent on specific hardware implementations\cite{b6}.

%%% =================================================================
%%% ======================= Emily Chang =============================

\subsection{Performance and Application Analysis of Vector Instruction Sets}

(a)Vector Instruction Sets Across Different Domains

RISC-V Vector Extension (RVV) and Arm Scalable Vector Extension (SVE), provide only one dimensional stride and random memory accesses. While this is sufficient for typical vector engines, it fails to effectively utilize the large Single Instruction, Multiple Data (SIMD) widths of in cache vector engines. This is because mobile data-parallel kernels expose limited parallelism across a single dimension. Therefore, vector instruction sets play a crucial role in modern computing architectures. By enabling parallel data processing, it improves computational efficiency, and accelerating various applications. These instruction sets have been widely adopted in high-performance computing (HPC), machine learning, and other computationally intensive domains \cite{b7}. 

For instance, multi-dimensional vector ISA Extension (MVE) for mobile in-cache computing uses dimension-level masked execution to reduce overhead in handling irregular parallelism. Additionally, it abstracts cache geometry and data layout, simplifying programming and enhancing efficiency to achieves high SIMD utilization (60\% vs. 23\% in RVV) and reduces instruction overhead.

(b)Performance Optimization in Different Application Scenarios

High-Performance Computing (HPC) provides libraries such as Basic Linear Algebra Subprograms (BLAS) which benefit from optimized vector processing. For instence, LINPACK-PC, a C implementation of the LINPACK benchmark, is available from Netlib9. This benchmark application makes use of several BLAS level 1 and 2 operations. It operates on matrices and vectors of dimensions up to 200x201. LINPACK-PC performs a certain sequence of operations in a loop for 5 seconds, and calculates the achieved MFLOPS from
the number of loop iterations \cite{b8}. 

(c)Performance Evaluation Methods

Gem5-AVX, an extended version of the Gem5 simulator that enables simulating recent x86 SIMD extensions, especially targeted for high performance computing (HPC). Gem5 is an open-source, event-driven computer architecture simulator used for modeling and simulating modern computer systems. Such as Intel AVX enhances the performance of SIMD instruction sets in prior generations by using a new instruction encoding scheme called the vector extension prefix (VEX). And it provides improved features that surpass those offered by previous 128-bit SIMD extentions \cite{b9}.



%%% ==============================================================
%%% ===================== Pin-Tzu Huang ==========================






%%%%%%% -- PAPER CONTENT ENDS -- %%%%%%%%


%%%%%%%%% -- BIB STYLE AND FILE -- %%%%%%%%
\begin{thebibliography}{00}
%%% ==================== Hsin-An Ko ====================
\bibitem{b1} H. Amiri and A. Shahbahrami, “SIMD programming using Intel vector extensions,” J. Parallel Distrib. Comput., vol. 135, pp. 83–100, Jan. 2020. doi: 10.1016/j.jpdc.2019.09.012.
\bibitem{b2} D. Kusswurm, "Modern X86 Assembly Language Programming: Covers X86 64-bit, AVX, AVX2, and AVX-512", Berkeley, CA: Apress, 2018. doi: 10.1007/978-1-4842-3005-6.
\bibitem{b3} P. Gepner, V. Gamayunov, and D. L. Fraser, "Early performance evaluation of AVX for HPC", International Conference on Computational Science, ICCS 2011.
\bibitem{b4} T. Odajima, Y. Kodama, and M. Sato, "Power Performance Analysis of ARM Scalable Vector Extension", 
\bibitem{b5} Yu. Kodama, T. Odajima, M. Matsuda, M. Tsuji, J. Lee, and M. Sato, "Preliminary Performance Evaluation of Application Kernels using ARM SVE with Multiple Vector Lengths", in 2017 IEEE International Conference on Cluster Computing (CLUSTER), 2017, pp. [page numbers]. doi: 10.1109/CLUSTER.2017.93.
\bibitem{b6} E. CUI ,T. LI, and Q. WE, "RISC-V Instruction Set Architecture Extensions: A Survey",  IEEE Access, vol. PP, no. 99, pp. 1-1, Jan. 2023. doi: 10.1109/ACCESS.2023.3246491.

%%%=======================================================
\bibitem{b7} N. Stephens et al., "The ARM Scalable Vector Extension," in IEEE Micro, vol. 37, no. 2, pp. 26-39, Mar.-Apr. 2017, doi: 10.1109/MM.2017.35.

\bibitem{b8} J. Kolodzey, "CRAY-1 Computer Technology," in IEEE Transactions on Components, Hybrids, and Manufacturing Technology, vol. 4, no. 2, pp. 181-186, June 1981, doi: 10.1109/TCHMT.1981.1135787.

\bibitem{b9} Roger Espasa, Mateo Valero, and James E. Smith. 1998. Vector architectures: past, present and future. In Proceedings of the 12th international conference on Supercomputing (ICS '98). Association for Computing Machinery, New York, NY, USA, 425–432. https://doi.org/10.1145/277830.277935

\bibitem{b10} A. Peleg and U. Weiser, "MMX technology extension to the Intel architecture," in IEEE Micro, vol. 16, no. 4, pp. 42-50, Aug. 1996, doi: 10.1109/40.526924.

\bibitem{b11} L. Kohn, G. Maturana, M. Tremblay, A. Prabhu and G. Zyner, "The visual instruction set (VIS) in UltraSPARC," Digest of Papers. COMPCON'95. Technologies for the Information Superhighway, San Francisco, CA, USA, 1995, pp. 462-469, doi: 10.1109/CMPCON.1995.512423.

\bibitem{b12} R. B. Lee, "Multimedia extensions for general-purpose processors," 1997 IEEE Workshop on Signal Processing Systems. SiPS 97 Design and Implementation formerly VLSI Signal Processing, Leicester, UK, 1997, pp. 9-23, doi: 10.1109/SIPS.1997.625683.

\bibitem{b13} S. Thakkur and T. Huff, "Internet Streaming SIMD Extensions," in Computer, vol. 32, no. 12, pp. 26-34, Dec. 1999, doi: 10.1109/2.809248.

\bibitem{b14} K. Diefendorff, P. K. Dubey, R. Hochsprung and H. Scale, "AltiVec extension to PowerPC accelerates media processing," in IEEE Micro, vol. 20, no. 2, pp. 85-95, March-April 2000, doi: 10.1109/40.848475.

\bibitem{b15} T. M. Conte et al., "Challenges to combining general-purpose and multimedia processors," in Computer, vol. 30, no. 12, pp. 33-37, Dec. 1997, doi: 10.1109/2.642799.

\bibitem{b16} M. Hassaballah, S. Omran and Y. B. Mahdy, "A Review of SIMD Multimedia Extensions and their Usage in Scientific and Engineering Applications," in The Computer Journal, vol. 51, no. 6, pp. 630-649, Nov. 2008, doi: 10.1093/comjnl/bxm099.

\bibitem{b17} D. Talla, L. K. John and D. Burger, "Bottlenecks in multimedia processing with SIMD style extensions and architectural enhancements," in IEEE Transactions on Computers, vol. 52, no. 8, pp. 1015-1031, Aug. 2003, doi: 10.1109/TC.2003.1223637.

%%% ==================== Emily Chang ====================

\bibitem{b18} A. Khadem, D. Fujiki, H. Chen, Y. Gu, N. Talati, S. Mahlke, and R. Das, “Multi-dimensional vector ISA extension for mobile in-cache computing,” arXiv preprint arXiv:2501.09902, 2025. [Online]. Available: https://arxiv.org/abs/2501.09902

\bibitem{b19} C. Fibich et al., "Evaluation of Open-Source Linear Algebra Libraries in Embedded Applications," 2019 8th Mediterranean Conference on Embedded Computing (MECO), Budva, Montenegro, 2019, pp. 1-6, doi: 10.1109/MECO.2019.8760041.

\bibitem{b20} S. Lee, Y. Kim, D. Nam and J. Kim, "Gem5-AVX: Extension of the Gem5 Simulator to Support AVX Instruction Sets," in IEEE Access, vol. 12, pp. 20767-20778, 2024, doi: 10.1109/ACCESS.2024.3359296.



\end{thebibliography}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

